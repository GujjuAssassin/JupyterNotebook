{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandTracker():\n",
    "    \n",
    "    def __init__(self, palm_model, joint_model, anchors_path,\n",
    "                box_enlarge=1.5, box_shift=0.2):\n",
    "        self.box_shift = box_shift\n",
    "        self.box_enlarge = box_enlarge\n",
    "\n",
    "        self.interp_palm = tf.lite.Interpreter(palm_model)\n",
    "        self.interp_palm.allocate_tensors()\n",
    "        self.interp_joint = tf.lite.Interpreter(joint_model)\n",
    "        self.interp_joint.allocate_tensors()\n",
    "\n",
    "        # reading the SSD anchors\n",
    "        with open(anchors_path, \"r\") as csv_f:\n",
    "            self.anchors = np.r_[\n",
    "                [x for x in csv.reader(csv_f, quoting=csv.QUOTE_NONNUMERIC)]\n",
    "            ]\n",
    "        # reading tflite model paramteres\n",
    "        output_details = self.interp_palm.get_output_details()\n",
    "        input_details = self.interp_palm.get_input_details()\n",
    "\n",
    "        self.in_idx = input_details[0]['index']\n",
    "        self.out_reg_idx = output_details[0]['index']\n",
    "        self.out_clf_idx = output_details[1]['index']\n",
    "\n",
    "        self.in_idx_joint = self.interp_joint.get_input_details()[0]['index']\n",
    "        self.out_idx_joint = self.interp_joint.get_output_details()[0]['index']\n",
    "\n",
    "        # 90Â° rotation matrix used to create the alignment triangle\n",
    "        self.R90 = np.r_[[[0,1],[-1,0]]]\n",
    "\n",
    "        # trianlge target coordinates used to move the detected hand\n",
    "        # into the right position\n",
    "        self._target_triangle = np.float32([\n",
    "                        [128, 128],\n",
    "                        [128,   0],\n",
    "                        [  0, 128]\n",
    "                    ])\n",
    "        self._target_box = np.float32([\n",
    "                        [  0,   0, 1],\n",
    "                        [256,   0, 1],\n",
    "                        [256, 256, 1],\n",
    "                        [  0, 256, 1],\n",
    "                    ])\n",
    "\n",
    "    def _get_triangle(self, kp0, kp2, dist=1):\n",
    "        \"\"\"get a triangle used to calculate Affine transformation matrix\"\"\"\n",
    "\n",
    "        dir_v = kp2 - kp0\n",
    "        dir_v /= np.linalg.norm(dir_v)\n",
    "\n",
    "        dir_v_r = dir_v @ self.R90.T\n",
    "        return np.float32([kp2, kp2+dir_v*dist, kp2 + dir_v_r*dist])\n",
    "\n",
    "    @staticmethod\n",
    "    def _triangle_to_bbox(source):\n",
    "        # plain old vector arithmetics\n",
    "        bbox = np.c_[\n",
    "            [source[2] - source[0] + source[1]],\n",
    "            [source[1] + source[0] - source[2]],\n",
    "            [3 * source[0] - source[1] - source[2]],\n",
    "            [source[2] - source[1] + source[0]],\n",
    "        ].reshape(-1,2)\n",
    "        return bbox\n",
    "\n",
    "    @staticmethod\n",
    "    def _im_normalize(img):\n",
    "         return np.ascontiguousarray(2 * ((img / 255) - 0.5).astype('float32'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigm(x):\n",
    "        return 1 / (1 + np.exp(-x) )\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad1(x):\n",
    "        return np.pad(x, ((0,0),(0,1)), constant_values=1, mode='constant')\n",
    "\n",
    "\n",
    "    def predict_joints(self, img_norm):\n",
    "        self.interp_joint.set_tensor(\n",
    "            self.in_idx_joint, img_norm.reshape(1,256,256,3))\n",
    "        self.interp_joint.invoke()\n",
    "\n",
    "        joints = self.interp_joint.get_tensor(self.out_idx_joint)\n",
    "        return joints.reshape(-1,2)\n",
    "\n",
    "    def detect_hand(self, img_norm):\n",
    "        assert -1 <= img_norm.min() and img_norm.max() <= 1,\\\n",
    "        \"img_norm should be in range [-1, 1]\"\n",
    "        assert img_norm.shape == (256, 256, 3),\\\n",
    "        \"img_norm shape must be (256, 256, 3)\"\n",
    "\n",
    "        # predict hand location and 7 initial landmarks\n",
    "        self.interp_palm.set_tensor(self.in_idx, img_norm[None])\n",
    "        self.interp_palm.invoke()\n",
    "\n",
    "        out_reg = self.interp_palm.get_tensor(self.out_reg_idx)[0]\n",
    "        out_clf = self.interp_palm.get_tensor(self.out_clf_idx)[0,:,0]\n",
    "        \n",
    "        detecion_mask = self._sigm(out_clf) > 0.7\n",
    "        candidate_detect = out_reg[detecion_mask]\n",
    "        candidate_anchors = self.anchors[detecion_mask]\n",
    "        ar=[]\n",
    "        for i in range(len(detecion_mask)):\n",
    "            if(detecion_mask[i]):\n",
    "                ar.append(i)\n",
    "        \n",
    "        if candidate_detect.shape[0] == 0:\n",
    "#             print(\"No hands found\")\n",
    "            return None, None\n",
    "        max_idx = np.argmax(candidate_detect[:, 3])\n",
    "\n",
    "        dx,dy,w,h = candidate_detect[max_idx, :4]\n",
    "        center_wo_offst = candidate_anchors[max_idx,:2] * 256\n",
    "        keypoints = center_wo_offst + candidate_detect[max_idx,4:].reshape(-1,2)\n",
    "        side = max(w,h) * self.box_enlarge\n",
    "        source = self._get_triangle(keypoints[0], keypoints[2], side)\n",
    "        source -= (keypoints[0] - keypoints[2]) * self.box_shift\n",
    "        return source, keypoints\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        # fit the image into a 256x256 square\n",
    "        shape = np.r_[img.shape]\n",
    "        pad = (shape.max() - shape[:2]).astype('uint32') // 2\n",
    "        img_pad = np.pad(\n",
    "            img,\n",
    "            ((pad[0],pad[0]), (pad[1],pad[1]), (0,0)),\n",
    "            mode='constant')\n",
    "        img_small = cv2.resize(img_pad, (256, 256))\n",
    "        img_small = np.ascontiguousarray(img_small)\n",
    "\n",
    "        img_norm = self._im_normalize(img_small)\n",
    "        return img_pad, img_norm, pad\n",
    "\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_pad, img_norm, pad = self.preprocess_img(img)\n",
    "\n",
    "        st=time.time()\n",
    "        source, keypoints = self.detect_hand(img_norm)\n",
    "        end = time.time()\n",
    "        if source is None:\n",
    "            return None, None, None\n",
    "\n",
    "        scale = max(img.shape) / 256\n",
    "        Mtr = cv2.getAffineTransform(\n",
    "            source * scale,\n",
    "            self._target_triangle\n",
    "        )\n",
    "        padd=self._im_normalize(img_pad)\n",
    "        img_landmark = cv2.warpAffine(\n",
    "            padd, Mtr, (256,256)\n",
    "        )\n",
    "        joints = self.predict_joints(img_landmark)\n",
    "\n",
    "        Mtr = self._pad1(Mtr.T).T\n",
    "        Mtr[2,:2] = 0\n",
    "\n",
    "        Minv = np.linalg.inv(Mtr)\n",
    "\n",
    "        kp_orig = (self._pad1(joints) @ Minv.T)[:,:2]\n",
    "        box_orig = (self._target_box @ Minv.T)[:,:2]\n",
    "        kp_orig -= pad[::-1]\n",
    "        box_orig -= pad[::-1]\n",
    "        \n",
    "        return kp_orig, box_orig, img_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from math import atan2,cos,sin\n",
    "%run gesLmUtilsLib.ipynb\n",
    "\n",
    "class StartGesRecog:\n",
    "\n",
    "    def __init__(self):\n",
    "        pd = tf.config.experimental.list_physical_devices('GPU')\n",
    "        tf.config.experimental.set_memory_growth(pd[0],True)\n",
    "\n",
    "        %run \"Gesture Prediction with Saimese.ipynb\"\n",
    "\n",
    "        self.WINDOW = \"Hand Tracking\"\n",
    "        PALM_MODEL_PATH = \"garbage/models/palm_detector.tflite\"\n",
    "        LANDMARK_MODEL_PATH = \"garbage/models/hand_landmark.tflite\"\n",
    "        ANCHORS_PATH = \"garbage/anchors.csv\"\n",
    "\n",
    "        self.POINT_COLOR = (0, 255, 0)\n",
    "        self.CONNECTION_COLOR = (255, 0, 0)\n",
    "        self.THICKNESS = 2\n",
    "\n",
    "        self.connections = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "            (5, 6), (6, 7), (7, 8),\n",
    "            (9, 10), (10, 11), (11, 12),\n",
    "            (13, 14), (14, 15), (15, 16),\n",
    "            (17, 18), (18, 19), (19, 20),\n",
    "            (0, 5), (5, 9), (9, 13), (13, 17), (0, 17)\n",
    "        ]\n",
    "\n",
    "        self.detector = HandTracker(\n",
    "            PALM_MODEL_PATH,\n",
    "            LANDMARK_MODEL_PATH,\n",
    "            ANCHORS_PATH,\n",
    "            box_shift=0.2,\n",
    "            box_enlarge=1.2\n",
    "        )\n",
    "\n",
    "        self.rotateit = lambda point: (int(point[0]*cos(ar)-point[1]*sin(ar)),int(point[0]*sin(ar)+point[1]*cos(ar)))\n",
    "        self.handlandmarks = []\n",
    "        self.newGestures = []\n",
    "        self.newGestureNames = []\n",
    "        self.counter = 0\n",
    "        self.reqlen = 0.2\n",
    "        self.trainData = True\n",
    "        self.gestureLabelsPath = \"garbage/GestureProject/GesLabels.txt\"\n",
    "        self.SiameseModelPath = \"garbage/GestureProject/models/GesSaimeseModel/iteration2\"\n",
    "        self.predefinedGesPath = \"garbage/GestureProject/TestingCoordinates/preprossed10RLTesting.csv\"\n",
    "        self.basePath = \"garbage/GestureProject/TestingCoordinates/\"\n",
    "        self.p = PredictLandmarks(ModelPath=self.SiameseModelPath,CompareGesPath=self.predefinedGesPath,trainData=self.trainData)\n",
    "        with open(self.gestureLabelsPath,\"r\") as labelFile:\n",
    "            import json\n",
    "            self.labels = json.load(labelFile)\n",
    "        print(\"Gestures Labels loaded from file:\" + self.gestureLabelsPath)\n",
    "\n",
    "    def startRecognizer(self,predict=True):\n",
    "        try:\n",
    "            capture = cv2.VideoCapture(0)\n",
    "\n",
    "            if capture.isOpened():\n",
    "                hasFrame, frame = capture.read()\n",
    "                frame = cv2.flip(frame,1)\n",
    "                forcomp=frame\n",
    "            else:\n",
    "                print(\"Cannot open camera\")\n",
    "                hasFrame = False\n",
    "\n",
    "            while hasFrame:\n",
    "\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                points, box, source = self.detector(image)\n",
    "\n",
    "                if points is not None:\n",
    "\n",
    "                    frame = impersonateLm(frame, points)\n",
    "\n",
    "                    if predict:\n",
    "                        prep = preprocessLm(points, normalize=True, trainData=self.trainData)\n",
    "                        label = self.labels[str(self.p.predict(prep))]\n",
    "                        blackBackground = np.zeros((480,640,3))\n",
    "                        xPoint = prep.reshape((21,2))*(480,640)\n",
    "                        cv2.putText(frame, label, (30,30), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255),1)\n",
    "                        blackBackground = impersonateLm(blackBackground, xPoint)\n",
    "                        cv2.imshow(\"Feeded data\", blackBackground)\n",
    "\n",
    "                cv2.imshow(self.WINDOW, frame)\n",
    "                hasFrame, frame = capture.read()\n",
    "                frame = cv2.flip(frame,1)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 121:\n",
    "                    self.handlandmarks.append(points)\n",
    "                    showLandmark(points)\n",
    "                    cv.waitKey(0)\n",
    "#                     addGes = input(\"Press y to add gesture in predefined ges list\")\n",
    "                    if key == 'y' or key == \"Y\":\n",
    "                        self.newGestures.append(point)\n",
    "                    print('coords added')\n",
    "                if key == 27:\n",
    "                    break\n",
    "            self.saveEmAll()\n",
    "        finally:\n",
    "            print(\"Closing cv windows and camera feed\")\n",
    "            capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    def addUniqueGes(self):\n",
    "        try:\n",
    "            capture = cv2.VideoCapture(0)\n",
    "\n",
    "            if capture.isOpened():\n",
    "                hasFrame, frame = capture.read()\n",
    "                frame = cv2.flip(frame,1)\n",
    "                forcomp=frame\n",
    "            else:\n",
    "                print(\"Cannot open camera\")\n",
    "                hasFrame = False\n",
    "\n",
    "            while hasFrame:\n",
    "\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                points, box, source = self.detector(image)\n",
    "\n",
    "                if points is not None:\n",
    "                    frame = impersonateLm(frame, points)\n",
    "\n",
    "                cv2.imshow(self.WINDOW, frame)\n",
    "                hasFrame, frame = capture.read()\n",
    "                frame = cv2.flip(frame,1)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27:\n",
    "                    break\n",
    "\n",
    "#                 If y is pressed\n",
    "                if key == 121:\n",
    "                    showLandmark(points)\n",
    "                    addGes = input(\"Press y to add gesture in predefined ges list\")\n",
    "                    if addGes == 'y' or addGes == 'Y':\n",
    "                        self.newGestures.append(points)\n",
    "                        name = input('Enter name for this gesture')\n",
    "                        self.newGestureNames.append(name)\n",
    "#                     print('coords added')    \n",
    "                    \n",
    "        finally:\n",
    "            print(\"Closing cv windows and camera feed\")\n",
    "            capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.saveEmAll()\n",
    "                        \n",
    "    def __appendGesName(self):\n",
    "        import json \n",
    "        with open(self.gestureLabelsPath,'r') as file:\n",
    "            file = json.load(file)\n",
    "            lastIndex = int(list(file.keys())[-1])+1\n",
    "            for gesName in self.newGestureNames:\n",
    "                lastIndex += 1\n",
    "                file[lastIndex] =  gesName\n",
    "        with open(self.gestureLabelsPath, 'w') as newFile:\n",
    "            json.dump(file, newFile)\n",
    "            \n",
    "            \n",
    "    def saveEmAll(self):\n",
    "        print(\"neGestures {}\".format(len(self.newGestures)))\n",
    "        print(\"neGestureNames {}\".format(len(self.newGestureNames)))\n",
    "        \n",
    "        if len(self.newGestures) is not 0 and len(self.newGestureNames) is not 0:\n",
    "            uniqueGes = preprocessLm(self.newGestures, normalize=True, trainData=True)\n",
    "            showLandmark(uniqueGes)\n",
    "            sure = input(\"Are you sure you want to Add this gestures: y for yes\")\n",
    "            if sure == 'y' or sure == 'Y':\n",
    "                from pandas import DataFrame,read_csv\n",
    "                assert uniqueGes.shape[-1] == (42) and len(uniqueGes.shape) == 2, \"Got array of shape {} instead of (-1,42)\".format(uniqueGes.shape)\n",
    "                allGes = read_csv(self.predefinedGesPath).to_numpy()\n",
    "                allGes = np.append(allGes,uniqueGes,axis=0)\n",
    "                print(allGes.shape)\n",
    "                DataFrame(allGes).to_csv(self.predefinedGesPath, index=False)\n",
    "                self.__appendGesName()\n",
    "                print(\"Done\")\n",
    "\n",
    "        if len(self.handlandmarks) is not 0:\n",
    "            key = input(\"press y to save Coordinates: \")\n",
    "            if key == 'y' or key == 'Y':\n",
    "                fileName = input('Enter Input file name')\n",
    "                filePath = self.basePath + fileName + '.csv'\n",
    "                print(\"Landmaks saved at [\" + filePath + \"]\")\n",
    "                import pandas as pd\n",
    "                lmarray = np.asarray(handlandmarks)\n",
    "                frame = pd.DataFrame(lmarray.reshape((-1,42)))\n",
    "                frame.to_csv(filePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[normalizeLm]: Got landmarks of shape 42. Reshaping to (21,2)\n",
      "[normalizeLm]: Got landmarks of shape 42. Reshaping to (21,2)\n",
      "Gestures Labels loaded from file:garbage/GestureProject/GesLabels.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Assessioner\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:72: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neGestures 0\n",
      "neGestureNames 0\n",
      "Closing cv windows and camera feed\n"
     ]
    }
   ],
   "source": [
    "x = StartGesRecog()\n",
    "# x.addUniqueGes()\n",
    "x.startRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#                 for point in points:\n",
    "#                     x, y = point\n",
    "#                     cv2.circle(frame, (int(x), int(y)), THICKNESS * 2, POINT_COLOR, THICKNESS)\n",
    "\n",
    "\n",
    "#                 for connection in connections:\n",
    "#                     x0, y0 = points[connection[0]]\n",
    "#                     x1, y1 = points[connection[1]]\n",
    "#                     cv2.line(frame, (int(x0), int(y0)), (int(x1), int(y1)), CONNECTION_COLOR, THICKNESS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
