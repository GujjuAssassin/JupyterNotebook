{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from math import atan2,cos,sin\n",
    "\n",
    "class HandTracker():\n",
    "    \n",
    "    def __init__(self, palm_model, joint_model, anchors_path,\n",
    "                box_enlarge=1.5, box_shift=0.2):\n",
    "        self.box_shift = box_shift\n",
    "        self.box_enlarge = box_enlarge\n",
    "\n",
    "        self.interp_palm = tf.lite.Interpreter(palm_model)\n",
    "        self.interp_palm.allocate_tensors()\n",
    "        self.interp_joint = tf.lite.Interpreter(joint_model)\n",
    "        self.interp_joint.allocate_tensors()\n",
    "\n",
    "        # reading the SSD anchors\n",
    "        with open(anchors_path, \"r\") as csv_f:\n",
    "            self.anchors = np.r_[\n",
    "                [x for x in csv.reader(csv_f, quoting=csv.QUOTE_NONNUMERIC)]\n",
    "            ]\n",
    "        # reading tflite model paramteres\n",
    "        output_details = self.interp_palm.get_output_details()\n",
    "        input_details = self.interp_palm.get_input_details()\n",
    "\n",
    "        self.in_idx = input_details[0]['index']\n",
    "        self.out_reg_idx = output_details[0]['index']\n",
    "        self.out_clf_idx = output_details[1]['index']\n",
    "\n",
    "        self.in_idx_joint = self.interp_joint.get_input_details()[0]['index']\n",
    "        self.out_idx_joint = self.interp_joint.get_output_details()[0]['index']\n",
    "\n",
    "        # 90Â° rotation matrix used to create the alignment triangle\n",
    "        self.R90 = np.r_[[[0,1],[-1,0]]]\n",
    "\n",
    "        # trianlge target coordinates used to move the detected hand\n",
    "        # into the right position\n",
    "        self._target_triangle = np.float32([\n",
    "                        [128, 128],\n",
    "                        [128,   0],\n",
    "                        [  0, 128]\n",
    "                    ])\n",
    "        self._target_box = np.float32([\n",
    "                        [  0,   0, 1],\n",
    "                        [256,   0, 1],\n",
    "                        [256, 256, 1],\n",
    "                        [  0, 256, 1],\n",
    "                    ])\n",
    "\n",
    "    def _get_triangle(self, kp0, kp2, dist=1):\n",
    "        \"\"\"get a triangle used to calculate Affine transformation matrix\"\"\"\n",
    "\n",
    "        dir_v = kp2 - kp0\n",
    "        dir_v /= np.linalg.norm(dir_v)\n",
    "\n",
    "        dir_v_r = dir_v @ self.R90.T\n",
    "        return np.float32([kp2, kp2+dir_v*dist, kp2 + dir_v_r*dist])\n",
    "\n",
    "    @staticmethod\n",
    "    def _triangle_to_bbox(source):\n",
    "        # plain old vector arithmetics\n",
    "        bbox = np.c_[\n",
    "            [source[2] - source[0] + source[1]],\n",
    "            [source[1] + source[0] - source[2]],\n",
    "            [3 * source[0] - source[1] - source[2]],\n",
    "            [source[2] - source[1] + source[0]],\n",
    "        ].reshape(-1,2)\n",
    "        return bbox\n",
    "\n",
    "    @staticmethod\n",
    "    def _im_normalize(img):\n",
    "         return np.ascontiguousarray(2 * ((img / 255) - 0.5).astype('float32'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigm(x):\n",
    "        return 1 / (1 + np.exp(-x) )\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad1(x):\n",
    "        return np.pad(x, ((0,0),(0,1)), constant_values=1, mode='constant')\n",
    "\n",
    "\n",
    "    def predict_joints(self, img_norm):\n",
    "        self.interp_joint.set_tensor(\n",
    "            self.in_idx_joint, img_norm.reshape(1,256,256,3))\n",
    "        self.interp_joint.invoke()\n",
    "\n",
    "        joints = self.interp_joint.get_tensor(self.out_idx_joint)\n",
    "        return joints.reshape(-1,2)\n",
    "\n",
    "    def detect_hand(self, img_norm):\n",
    "        assert -1 <= img_norm.min() and img_norm.max() <= 1,\\\n",
    "        \"img_norm should be in range [-1, 1]\"\n",
    "        assert img_norm.shape == (256, 256, 3),\\\n",
    "        \"img_norm shape must be (256, 256, 3)\"\n",
    "\n",
    "        # predict hand location and 7 initial landmarks\n",
    "        self.interp_palm.set_tensor(self.in_idx, img_norm[None])\n",
    "        self.interp_palm.invoke()\n",
    "\n",
    "        out_reg = self.interp_palm.get_tensor(self.out_reg_idx)[0]\n",
    "        out_clf = self.interp_palm.get_tensor(self.out_clf_idx)[0,:,0]\n",
    "        \n",
    "        detecion_mask = self._sigm(out_clf) > 0.7\n",
    "        candidate_detect = out_reg[detecion_mask]\n",
    "        candidate_anchors = self.anchors[detecion_mask]\n",
    "        ar=[]\n",
    "        for i in range(len(detecion_mask)):\n",
    "            if(detecion_mask[i]):\n",
    "                ar.append(i)\n",
    "        \n",
    "        if candidate_detect.shape[0] == 0:\n",
    "#             print(\"No hands found\")\n",
    "            return None, None\n",
    "        max_idx = np.argmax(candidate_detect[:, 3])\n",
    "\n",
    "        dx,dy,w,h = candidate_detect[max_idx, :4]\n",
    "        center_wo_offst = candidate_anchors[max_idx,:2] * 256\n",
    "        keypoints = center_wo_offst + candidate_detect[max_idx,4:].reshape(-1,2)\n",
    "        side = max(w,h) * self.box_enlarge\n",
    "        source = self._get_triangle(keypoints[0], keypoints[2], side)\n",
    "        source -= (keypoints[0] - keypoints[2]) * self.box_shift\n",
    "        return source, keypoints\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        # fit the image into a 256x256 square\n",
    "        shape = np.r_[img.shape]\n",
    "        pad = (shape.max() - shape[:2]).astype('uint32') // 2\n",
    "        img_pad = np.pad(\n",
    "            img,\n",
    "            ((pad[0],pad[0]), (pad[1],pad[1]), (0,0)),\n",
    "            mode='constant')\n",
    "        img_small = cv2.resize(img_pad, (256, 256))\n",
    "        img_small = np.ascontiguousarray(img_small)\n",
    "\n",
    "        img_norm = self._im_normalize(img_small)\n",
    "        return img_pad, img_norm, pad\n",
    "\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_pad, img_norm, pad = self.preprocess_img(img)\n",
    "\n",
    "        st=time.time()\n",
    "        source, keypoints = self.detect_hand(img_norm)\n",
    "        end = time.time()\n",
    "        if source is None:\n",
    "            return None, None, None\n",
    "\n",
    "        scale = max(img.shape) / 256\n",
    "        Mtr = cv2.getAffineTransform(\n",
    "            source * scale,\n",
    "            self._target_triangle\n",
    "        )\n",
    "        padd=self._im_normalize(img_pad)\n",
    "        img_landmark = cv2.warpAffine(\n",
    "            padd, Mtr, (256,256)\n",
    "        )\n",
    "        joints = self.predict_joints(img_landmark)\n",
    "\n",
    "        Mtr = self._pad1(Mtr.T).T\n",
    "        Mtr[2,:2] = 0\n",
    "\n",
    "        Minv = np.linalg.inv(Mtr)\n",
    "\n",
    "        kp_orig = (self._pad1(joints) @ Minv.T)[:,:2]\n",
    "        box_orig = (self._target_box @ Minv.T)[:,:2]\n",
    "        kp_orig -= pad[::-1]\n",
    "        box_orig -= pad[::-1]\n",
    "        \n",
    "        return kp_orig, box_orig, img_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'One', '1': 'Two', '2': 'Three', '3': 'Four', '4': 'Five', '5': 'Tasty', '6': 'SpiderMan', '7': 'SuperMan', '8': 'ThumbsUp', '9': 'ThumbsDown'}\n",
      "[normalizeLm]: Got landmarks of shape 42. Reshaping to (21,2)\n",
      "[normalizeLm]: Got landmarks of shape 42. Reshaping to (21,2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Assessioner\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:79: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "pd = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(pd[0],True)\n",
    "\n",
    "%run \"Gesture Prediction with Saimese.ipynb\"\n",
    "\n",
    "WINDOW = \"Hand Tracking\"\n",
    "PALM_MODEL_PATH = \"garbage/models/palm_detector.tflite\"\n",
    "LANDMARK_MODEL_PATH = \"garbage/models/hand_landmark.tflite\"\n",
    "ANCHORS_PATH = \"garbage/anchors.csv\"\n",
    "\n",
    "POINT_COLOR = (0, 255, 0)\n",
    "CONNECTION_COLOR = (255, 0, 0)\n",
    "THICKNESS = 2\n",
    "\n",
    "cv2.namedWindow(WINDOW)\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if capture.isOpened():\n",
    "    hasFrame, frame = capture.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    forcomp=frame\n",
    "else:\n",
    "    hasFrame = False\n",
    "\n",
    "connections = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "    (5, 6), (6, 7), (7, 8),\n",
    "    (9, 10), (10, 11), (11, 12),\n",
    "    (13, 14), (14, 15), (15, 16),\n",
    "    (17, 18), (18, 19), (19, 20),\n",
    "    (0, 5), (5, 9), (9, 13), (13, 17), (0, 17)\n",
    "]\n",
    "\n",
    "detector = HandTracker(\n",
    "    PALM_MODEL_PATH,\n",
    "    LANDMARK_MODEL_PATH,\n",
    "    ANCHORS_PATH,\n",
    "    box_shift=0.2,\n",
    "    box_enlarge=1.2\n",
    ")\n",
    "\n",
    "with open('garbage/GestureProject/GestureLabels.txt',\"r\") as labelFile:\n",
    "    import json\n",
    "    labels = json.load(labelFile)\n",
    "print(labels)\n",
    "\n",
    "rotateit = lambda point: (int(point[0]*cos(ar)-point[1]*sin(ar)),int(point[0]*sin(ar)+point[1]*cos(ar)))\n",
    "handlandmarks = []\n",
    "counter = 0\n",
    "reqlen = 0.2\n",
    "trainData = True\n",
    "p = PredictLandmarks(ModelPath=\"garbage/GestureProject/models/GesSaimeseModel/iteration2\",CompareGesPath=\"garbage/GestureProject/RawGestures/RelocatedRawGes10R.csv\",trainData=trainData)\n",
    "\n",
    "while hasFrame:\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    points, box, source = detector(image)\n",
    "    \n",
    "    if points is not None:\n",
    "        \n",
    "        for point in points:\n",
    "            x, y = point\n",
    "            cv2.circle(frame, (int(x), int(y)), THICKNESS * 2, POINT_COLOR, THICKNESS)\n",
    "            \n",
    "        \n",
    "        for connection in connections:\n",
    "            x0, y0 = points[connection[0]]\n",
    "            x1, y1 = points[connection[1]]\n",
    "            cv2.line(frame, (int(x0), int(y0)), (int(x1), int(y1)), CONNECTION_COLOR, THICKNESS)\n",
    "        \n",
    "        \n",
    "#         if p is defined the only predict and show result as well as landmark\n",
    "        if 'p' in vars():\n",
    "            prep = preprocessLm(points, normalize=True, trainData=trainData)\n",
    "            label = labels[str(p.predict(prep))]\n",
    "            blackBackground = np.zeros((480,640,3))\n",
    "            xPoint = prep.reshape((21,2))*(480,640)\n",
    "            cv2.putText(frame, label, (30,30), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255),1)\n",
    "\n",
    "            for point in xPoint:\n",
    "                x, y = point\n",
    "                cv2.circle(blackBackground, (int(x), int(y)), THICKNESS * 2, POINT_COLOR, THICKNESS)\n",
    "\n",
    "            for connection in connections:\n",
    "                x0, y0 = xPoint[connection[0]]\n",
    "                x1, y1 = xPoint[connection[1]]\n",
    "                cv2.line(blackBackground, (int(x0), int(y0)), (int(x1), int(y1)), CONNECTION_COLOR, THICKNESS)\n",
    "\n",
    "            cv2.imshow(\"Feeded data\", blackBackground)\n",
    "        \n",
    "        \n",
    "    cv2.imshow(WINDOW, frame)\n",
    "    hasFrame, frame = capture.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 121:\n",
    "        handlandmarks.append(points)\n",
    "        print('coords added')\n",
    "    if key == 27:\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if len(handlandmarks) is not 0:\n",
    "    key = input(\"press y to save Coordinates: \")\n",
    "    if key == 'y' or key == 'Y':\n",
    "        basePath = \"garbage/GestureProject/TestingCoordinates/\"\n",
    "        fileName = input('Enter Input file name')\n",
    "        filePath = basePath + fileName + '.csv'\n",
    "        print(\"Landmaks saved at [\" + filePath + \"]\")\n",
    "        import pandas as pd\n",
    "        lmarray = np.asarray(handlandmarks)\n",
    "        frame = pd.DataFrame(lmarray.reshape((-1,42)))\n",
    "        frame.to_csv(filePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = {\n",
    "#         0:\"One\",\n",
    "#         1:\"Two\",\n",
    "#         2:\"Three\",\n",
    "#         3:\"Four\",\n",
    "#         4:\"Five\",\n",
    "#         5:\"Tasty\",\n",
    "#         6:\"SpiderMan\",\n",
    "#         7:\"SuperMan\",\n",
    "#         8:\"ThumbsUp\"\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
